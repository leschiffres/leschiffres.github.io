[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthaios Letsios",
    "section": "",
    "text": "Hi there ðŸ‘‹\nMy name is Matthaios Letsios and in the past years I have been working in the field of Data Engineering and Data Science. List of my responsibilities in my previous roles have been to:\n\nMaintain, develop and enhance airflow pipelines focusing proper data collection, transformation and storage in the most efficient manner.\nDevelop python services using technologies like gRPC, postgreSQL, clickhouse, redis and kafka.\nDevelop and maintain NLP services.\nDeveloping different machine learning models (mainly anomaly detection).\n\n\n\nResearch\nIn the past I have also worked as a researcher in Telecom ParisTech & Universite Pierre Marie Curie.\nIâ€™m also proud of being co-author to the following publications:\n\nFinding Heaviest k-Subgraphs and Events in Social Media. ICDM Workshops 2016 link\nScheduling under Uncertainty: A Query-based Approach. IJCAI 2018 link\n\n\n\nFun Projects\nUnderstats Scraping\n \nunderstat.com is a website providing advanced data for football matches e.g.Â xGoals, xAssists, xGChain etc. In this project I set up an airflow instance to collect the data for the Premier League football matches, transform them and then in the end visualize them in a jupyter notebook. The motivation behind this is the Fantasy Premier League game, where I use those data as a basis for the weekly player selection.\nBook Recommender\n \nThis is an web app that makes use of sentence embeddings to be able to find appropriate books, based on the given desired description. The webapp was build using Fastapi.\n\n\nGoogle Haschode\nIn the past I have participated in many google hashcode competitions. Unfortunately the competition no longer exists, but I hope Google will bring it back in the future. Here is the code for some of my participations.\nGoogle Hashcode 2021 - Qualification Round\n \nProblem: Given a city plan and all car itineraries in that city, the goal is to schedule all traffic lights, to help as many cars as possible to reach their destination on time.\nApproach: Our solution takes into account the in & out degree of each traffic light and assigns proportinally the time to each traffic light. Also we find out that assigning to all traffic lights 1 second of traffic time provided efficient solutions in some instances. This solution allowed us to reach top 20% of the leaderboard during the competition."
  },
  {
    "objectID": "mddocs/ml_and_stats/likelihood.html",
    "href": "mddocs/ml_and_stats/likelihood.html",
    "title": "Likelihood Maximization (Draft)",
    "section": "",
    "text": "Letâ€™s assume you have some data. To analyze them we would most probably try to estimate the mean or the standard deviation. However we would be able to extract even more useful information about the data if we would be able to estimate the probability density function.\nOverall we want to estimate the \\(p(x|\\theta)\\), by finding the optimal values of \\(\\theta\\). If we have multiple candidate models, how can we decide what is the best one? This is what likelihood is used for. x\nIf we have the following data \\(x_1,x_2 \\ldots x_n\\) then the probability that they all came from a specific distribution with parameters \\(\\theta\\) is:\n\\[p(x|\\theta) = \\prod_{i=1}^n p(x_i|\\theta)\\]\nThe likelihood is a function of \\(\\theta\\) and by maximizing it we find the optimal values. Since the multiplication of all of those probabilities can cause problems in the computation (because of the representation of float numbers in a computer), we try to maximize the \\(logp(x|\\theta)\\) which will lead to the same value. It is denoted by \\(L(\\theta)\\)\n\\[L(\\theta) = logp(x|\\theta) = \\sum_{i=1}^{n} logp(x_i|theta)\\]"
  },
  {
    "objectID": "mddocs/ml_and_stats/likelihood.html#applying-the-maximum-likelihood-estimation-method",
    "href": "mddocs/ml_and_stats/likelihood.html#applying-the-maximum-likelihood-estimation-method",
    "title": "Likelihood Maximization (Draft)",
    "section": "Applying the maximum likelihood estimation method",
    "text": "Applying the maximum likelihood estimation method\nif we tried to maximize the likelihood estimation for the normal distribution we would get that this is possible.\n\nby maximizing for \\(\\mu\\): \\(\\frac{dL}{d\\mu} = 0\\) then we would get that \\(\\mu = \\frac{1}{n} \\sum_{i=1}^nx_i\\)\nby maximizing for \\(\\sigma\\) \\(\\frac{dL}{d\\sigma^2} = 0\\) then we would get that \\(\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\\)\n\nfor the exponential distribution:\n\\(\\frac{dL}{d\\lambda} = 0\\) then we get \\(\\lambda = \\frac{n}{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{x}}\\)\nfor the bernoulli distribution where \\(p(x|\\pi) = \\pi^x(1-\\pi)^{1-x_i}\\) we would get:\n\\(\\frac{dL}{d\\pi} = 0\\) then we get \\(\\pi = \\frac{1}{n}\\sum_{i=1}^n x_i\\)"
  },
  {
    "objectID": "mddocs/ml_and_stats/likelihood.html#maximum-likelihood-estimation-using-a-mixture-of-distributions",
    "href": "mddocs/ml_and_stats/likelihood.html#maximum-likelihood-estimation-using-a-mixture-of-distributions",
    "title": "Likelihood Maximization (Draft)",
    "section": "Maximum likelihood estimation using a mixture of distributions",
    "text": "Maximum likelihood estimation using a mixture of distributions\n\\[p(x|\\theta) = \\sum_{k=1}^K w_k p_k(x|\\theta_k)\\]\ne.g Gaussian Mixture Models"
  },
  {
    "objectID": "mddocs/ml_and_stats/hypothesis-testing.html",
    "href": "mddocs/ml_and_stats/hypothesis-testing.html",
    "title": "Hypothesis Testing (Draft)",
    "section": "",
    "text": "Hypothesis or statistical testing is a really useful method in statistics that allows us to investigate and measure the validity of our assumptions.\nLetâ€™s assume that a basketball player claims that his free throw percentage is \\(85\\%\\) and thatâ€™s why we challenged him to prove it. After 50 free throws he ended up scoring 38 of them leading to a percentage of \\(76\\%\\). Can we safely assume that this person is lying and that his actual ratio is lower than \\(85\\%\\)?\nOne way to test something like that is to use a computer to run a simulation several times and see how many times we actually had a percentage less or equal than \\(76\\%\\). Indeed we tested it by running 1000 times a binomial distribution with \\(p=0.85\\) and it seems that only 22 out of 1000 trials such a thing happened. A \\(2.2\\%\\) seems to be really small and maybe that is an indicator that in fact this person lies.\nWe could also compute the confidence interval for such an experiment is \\((0.751\\%, 0.949\\%)\\) which is quite large interval, but it seems that 75% is already out of the confidence interval.\n\n\n\nfree throws\n\n\nStatisticians have streamlined this process by introducing what is called Statistical Hypothesis Testing.\nIn this context the first step is to make a hypothesis of exactly the opposite of what we are trying to prove. This is called the Null Hypothesis denoted by \\(H_0\\) and an Alternative Hypothesis denoted by \\(H_1\\). e.g.Â in the above case the null hypothesis would be that We assume that normal free throw ratio is \\(\\mu = \\mu_0 = 85\\%\\) and the alternative hypothesis is that the normal free throw ratio is \\(\\mu &lt; 85\\%\\).\nThe second step is to calculate a test-function. Where we compute an unbiased sampled mean \\(\\bar{\\mu}\\) and we compute the statistical function: \\[ z = \\frac{\\bar{\\mu} - \\mu_0}{s_{\\bar{\\mu}}} \\]\nThe \\(z\\) function is a regularized value. If the Null Hypothesis was true, then \\(z\\) follows the standard normal distribution. The further \\(z\\) is from 0 then the higher unlikely is that the null hypothesis is not true.\nThatâ€™s where the third step comes into play where we have to compute the P-value. P-value is the probability that this test function has such a value considering what we were able to sample.\nThe fourth step is to decide if we are going to reject the null hypothesis based on the \\(P-value\\) we calculated.\n\n\\(P-value &lt; \\alpha\\): We reject the null hypothesis\n\\(P-value \\geq \\alpha\\): We fail reject the null hypothesis\n\nwhere \\(\\alpha\\) is the significance level of the test.\n\nPlaces met\n\nLinear regression: P-value indicating whether a predictor is relevant or not with the target variable\nAB-test: Compare the means between two different populations\n\n\n\nWell known tests\n\nT-test\n\nIndependent Samples T-Test: Used when comparing the means of two independent groups. For example, you might use it to compare the test scores of two different classes.\nPaired Samples T-Test: Used when comparing the means of related groups, often before and after some intervention. For instance, you could use it to compare the weights of individuals before and after a weight loss program.\nOne-Sample T-Test: Used to determine if the mean of a single sample differs significantly from a known or hypothesized population mean\n\nChi-Square Test: Used to assess the association between categorical variables. Itâ€™s often used for comparing observed and expected frequencies in a contingency table.\nANOVA (Analysis of Variance): Used to compare means among more than two groups. It tells you whether there are statistically significant differences between the means of multiple groups."
  },
  {
    "objectID": "ml_and_stats.html",
    "href": "ml_and_stats.html",
    "title": "Machine Learning and Statistics",
    "section": "",
    "text": "Here I add a bunch of personal notes on Machine Learning and Statistics I have collected over the years.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLikelihood Maximization (Draft)\n\n\n\n\n\n\n\nStatistics\n\n\nLikelihood\n\n\n\n\nWhat is Likelihood? How can it be used? What problems can it help us solve?\n\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHypothesis Testing (Draft)\n\n\n\n\n\n\n\nStatistics\n\n\nHypothesis Testing\n\n\n\n\nHow can you prove if your friend is lying about his basketball shooting skills?\n\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\nNo matching items"
  }
]