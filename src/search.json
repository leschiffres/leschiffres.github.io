[
  {
    "objectID": "book_reviews.html",
    "href": "book_reviews.html",
    "title": "Book Reviews",
    "section": "",
    "text": "Here I have added the summary and main takeaways of books I have read over the years.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/ml_and_stats/likelihood.html",
    "href": "docs/ml_and_stats/likelihood.html",
    "title": "Likelihood Maximization (Test)",
    "section": "",
    "text": "What is likelihood?\nHow it can be used?\nWhat problems can it help us solve?\n\nLet’s assume you have some data. To analyze them we would most probably try to estimate the mean or the standard deviation. However we would be able to extract even more useful information about the data if we would be able to estimate the probability density function.\nOverall we want to estimate the \\(p(x|\\theta)\\), by finding the optimal values of \\(\\theta\\). If we have multiple candidate models, how can we decide what is the best one? This is what likelihood is used for. x\n\n\n\nThe likelihood expresses a way to determine how probable it is that the data came from a distribution with specific parameters.\n\n\n\n\nNormal Distribution: \\(p(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x-\\mu^2)}\\)\nExponential Distribution: \\(p(x|\\lambda) = \\lambda e^{-\\lambda}\\)\n\nIf we have the following data \\(x_1,x_2 \\ldots x_n\\) then the probability that they all came from a specific distribution with parameters \\(\\theta\\) is:\n\\[p(x|\\theta) = \\prod_{i=1}^n p(x_i|\\theta)\\]\nThe likelihood is a function of \\(\\theta\\) and by maximizing it we find the optimal values. Since the multiplication of all of those probabilities can cause problems in the computation (because of the representation of float numbers in a computer), we try to maximize the \\(logp(x|\\theta)\\) which will lead to the same value. It is denoted by \\(L(\\theta)\\)\n\\[L(\\theta) = logp(x|\\theta) = \\sum_{i=1}^{n} logp(x_i|theta)\\]\n\n\nif we tried to maximize the likelihood estimation for the normal distribution we would get that this is possible.\n\nby maximizing for \\(\\mu\\): \\(\\frac{dL}{d\\mu} = 0\\) then we would get that \\(\\mu = \\frac{1}{n} \\sum_{i=1}^nx_i\\)\nby maximizing for \\(\\sigma\\) \\(\\frac{dL}{d\\sigma^2} = 0\\) then we would get that \\(\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\\)\n\nfor the exponential distribution:\n\\(\\frac{dL}{d\\lambda} = 0\\) then we get \\(\\lambda = \\frac{n}{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{x}}\\)\nfor the bernoulli distribution where \\(p(x|\\pi) = \\pi^x(1-\\pi)^{1-x_i}\\) we would get:\n\\(\\frac{dL}{d\\pi} = 0\\) then we get \\(\\pi = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\n\n\n\n\\[p(x|\\theta) = \\sum_{k=1}^K w_k p_k(x|\\theta_k)\\]\ne.g Gaussian Mixture Models"
  },
  {
    "objectID": "docs/ml_and_stats/likelihood.html#applying-the-maximum-likelihood-estimation-method",
    "href": "docs/ml_and_stats/likelihood.html#applying-the-maximum-likelihood-estimation-method",
    "title": "Likelihood Maximization (Test)",
    "section": "",
    "text": "if we tried to maximize the likelihood estimation for the normal distribution we would get that this is possible.\n\nby maximizing for \\(\\mu\\): \\(\\frac{dL}{d\\mu} = 0\\) then we would get that \\(\\mu = \\frac{1}{n} \\sum_{i=1}^nx_i\\)\nby maximizing for \\(\\sigma\\) \\(\\frac{dL}{d\\sigma^2} = 0\\) then we would get that \\(\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\\)\n\nfor the exponential distribution:\n\\(\\frac{dL}{d\\lambda} = 0\\) then we get \\(\\lambda = \\frac{n}{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{x}}\\)\nfor the bernoulli distribution where \\(p(x|\\pi) = \\pi^x(1-\\pi)^{1-x_i}\\) we would get:\n\\(\\frac{dL}{d\\pi} = 0\\) then we get \\(\\pi = \\frac{1}{n}\\sum_{i=1}^n x_i\\)"
  },
  {
    "objectID": "docs/ml_and_stats/likelihood.html#maximum-likelihood-estimation-using-a-mixture-of-distributions",
    "href": "docs/ml_and_stats/likelihood.html#maximum-likelihood-estimation-using-a-mixture-of-distributions",
    "title": "Likelihood Maximization (Test)",
    "section": "",
    "text": "\\[p(x|\\theta) = \\sum_{k=1}^K w_k p_k(x|\\theta_k)\\]\ne.g Gaussian Mixture Models"
  },
  {
    "objectID": "ml_and_stats.html",
    "href": "ml_and_stats.html",
    "title": "Machine Learning and Statistics",
    "section": "",
    "text": "Here I add a bunch of personal notes on Machine Learning and Statistics I have collected over the years.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLikelihood Maximization (Test)\n\n\n\n\n\n\n\nStatistics\n\n\nLikelihood\n\n\n\n\nWhat is Likelihood? How can it be used? What problems can it help us solve?\n\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHypothesis Testing (Test)\n\n\n\n\n\n\n\nStatistics\n\n\nHypothesis Testing\n\n\n\n\nHypothesis & Statistical Testing explained\n\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/ml_and_stats/hypothesis-testing.html",
    "href": "docs/ml_and_stats/hypothesis-testing.html",
    "title": "Hypothesis Testing (Test)",
    "section": "",
    "text": "Hypothesis Testing\nHypothesis or statistical testing is a really useful method in statistics that allows us to investigate and measure the validity of our assumptions.\nLet’s assume that a basketball player claims that his free throw percentage is \\(85\\%\\) and that’s why we challenged him to prove it. After 50 free throws he ended up scoring 38 of them leading to a percentage of \\(76\\%\\). Can we safely assume that this person is lying and that his actual ratio is lower than \\(85\\%\\)?\nOne way to test something like that is to use a computer to run a simulation several times and see how many times we actually had a percentage less or equal than \\(76\\%\\). Indeed we tested it by running 1000 times a binomial distribution with \\(p=0.85\\) and it seems that only 22 out of 1000 trials such a thing happened. A \\(2.2\\%\\) seems to be really small and maybe that is an indicator that in fact this person lies.\nWe could also compute the confidence interval for such an experiment is \\((0.751\\%, 0.949\\%)\\) which is quite large interval, but it seems that 75% is already out of the confidence interval.\n\n\n\nfree throws\n\n\nStatisticians have streamlined this process by introducing what is called Statistical Hypothesis Testing.\nIn this context the first step is to make a hypothesis of exactly the opposite of what we are trying to prove. This is called the Null Hypothesis denoted by \\(H_0\\) and an Alternative Hypothesis denoted by \\(H_1\\). e.g. in the above case the null hypothesis would be that We assume that normal free throw ratio is \\(\\mu = \\mu_0 = 85\\%\\) and the alternative hypothesis is that the normal free throw ratio is \\(\\mu &lt; 85\\%\\).\nThe second step is to calculate a test-function. Where we compute an unbiased sampled mean \\(\\bar{\\mu}\\) and we compute the statistical function: \\[ z = \\frac{\\bar{\\mu} - \\mu_0}{s_{\\bar{\\mu}}} \\]\nThe \\(z\\) function is a regularized value. If the Null Hypothesis was true, then \\(z\\) follows the standard normal distribution. The further \\(z\\) is from 0 then the higher unlikely is that the null hypothesis is not true.\nThat’s where the third step comes into play where we have to compute the P-value. P-value is the probability that this test function has such a value considering what we were able to sample.\nThe fourth step is to decide if we are going to reject the null hypothesis based on the \\(P-value\\) we calculated.\n\n\\(P-value &lt; \\alpha\\): We reject the null hypothesis\n\\(P-value \\geq \\alpha\\): We fail reject the null hypothesis\n\nwhere \\(\\alpha\\) is the significance level of the test.\n\n\nPlaces met\n\nLinear regression: P-value indicating whether a predictor is relevant or not with the target variable\nAB-test: Compare the means between two different populations\n\n\n\nWell known tests\n\nT-test\n\nIndependent Samples T-Test: Used when comparing the means of two independent groups. For example, you might use it to compare the test scores of two different classes.\nPaired Samples T-Test: Used when comparing the means of related groups, often before and after some intervention. For instance, you could use it to compare the weights of individuals before and after a weight loss program.\nOne-Sample T-Test: Used to determine if the mean of a single sample differs significantly from a known or hypothesized population mean\n\nChi-Square Test: Used to assess the association between categorical variables. It’s often used for comparing observed and expected frequencies in a contingency table.\nANOVA (Analysis of Variance): Used to compare means among more than two groups. It tells you whether there are statistically significant differences between the means of multiple groups."
  }
]